# -*- coding: utf-8 -*-
"""Finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yhdmN7E_Z_zgb12wsIQi4-1RQ7vvE5aW
"""

import pandas as pd
import numpy as np
df=pd.read_csv('/content/student-mat.csv')
df

df=df.dropna()
df

column_names = df.columns.tolist()
print(column_names)

for column in df.select_dtypes(include=['int', 'float']):
    print(f"----- {column} -----")
    print(df[column].describe())
    print("\n")

for column in df.select_dtypes(exclude=['int', 'float']):
    print(f"----- {column} -----")
    print("Unique values:", df[column].unique())
    print("\nValue Counts:")
    print(df[column].value_counts())
    print("\n")

"""# Random forest"""

df= df[pd.to_numeric(df['G1'], errors='coerce').notna()]

#Data processing for maching learning
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'schoolsup', 'famsup',
    'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'Mjob',
    'Fjob', 'reason', 'guardian']
numerical_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',
    'freetime', 'goout', 'Dalc', 'Walc', 'health','absences']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
        ])
# 假设 df 是您的原始 DataFrame
X = df[categorical_cols + numerical_cols]  # 选择需要预处理的列
X_transformed = preprocessor.fit_transform(X)
# 获取哑变量列名
ohe_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)

# 合并所有列名
all_columns = ohe_columns.tolist() + numerical_cols

# 创建新的 DataFrame
X_transformed_df = pd.DataFrame(X_transformed, columns=all_columns)

X_transformed_df=X_transformed_df.drop(['school_GP','sex_M','address_U','famsize_GT3','Pstatus_T','schoolsup_no','famsup_no','paid_no','activities_no','nursery_no','higher_no','internet_no','romantic_no','Mjob_other','Fjob_other','reason_other','guardian_other'],axis=1)
X_transformed_df.columns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = X_transformed_df
Y2 = df['G3']
#Covert Y1 to binary
def convert_to_binary(note):
    return 1 if note >= 10 else 0
y1_clf =Y2.apply(convert_to_binary)

X_train_por, X_test_por, y_train_por, y_test_por = train_test_split(X, y1_clf, test_size=0.3, random_state=42)

# Create RandomForestClassifier
rf_por_model = RandomForestClassifier(random_state=42)

# Train the model
rf_por_model.fit(X_train_por, y_train_por)

# Predictions on the test set
test_predictions = rf_por_model.predict(X_test_por)

# Evaluate the model
test_accuracy = accuracy_score(y_test_por, test_predictions)

print("Testing Accuracy:", test_accuracy)

# Additional evaluation metrics
print("\nClassification Report on Test Set:\n", classification_report(y_test_por, test_predictions))
print("\nConfusion Matrix on Test Set:\n", confusion_matrix(y_test_por, test_predictions))

import matplotlib.pyplot as plt

# Get feature importances from the trained model
feature_importances = rf_por_model.feature_importances_

# Get the indices of the top ten features
top_ten_indices = feature_importances.argsort()[-10:][::-1]

# Extract the names and importances of the top ten features
top_ten_features = X_transformed_df.columns[top_ten_indices]
top_ten_importances = feature_importances[top_ten_indices]

plt.figure(figsize=(12, 8))
# Plotting only the top ten features
plt.bar(top_ten_features, top_ten_importances, color='lightblue')
plt.title('Top Ten Random Forest Feature Importances')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

fi = pd.DataFrame({'names': X_transformed_df.columns, 'fi': feature_importances})
fi = fi.sort_values(by='fi',ascending = False)
fi.head(10)

"""# **relationship mining**"""

df1 = df[['schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']]
df1

# Define the mapping for replacements
replacement_mapping = {
    'yes': 1,
    'no': 0
}

# Replace values in the DataFrame
df1.replace(replacement_mapping, inplace=True)
df1

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
frequent_itemsets = apriori(df1, min_support=0.5, use_colnames=True)
Association = association_rules(frequent_itemsets,metric="confidence", min_threshold=0.7)
Association

filtered_df = Association[Association['lift'] > 1]
Association_sorted = filtered_df.sort_values(by=['lift'], ascending=[False])
Association_sorted

import networkx as nx
import matplotlib.pyplot as plt

G = nx.DiGraph()

for _, row in Association_sorted.iterrows():
    antecedents_str = ', '.join(map(str, row['antecedents']))
    consequents_str = ', '.join(map(str, row['consequents']))
    G.add_edge(antecedents_str, consequents_str, weight=row['lift'])

def edge_color(lift_value):
    return 'black'  # Set edge color to black

plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)

# Increase arrow size by adjusting the mutation_scale parameter
arrow_size = 25  # You can adjust this value based on your preference

node_size = 700  # You can adjust this value based on your preference

# Draw nodes with specified size
nx.draw_networkx_nodes(G, pos, node_size=node_size)


# Draw labels
nx.draw_networkx_labels(G, pos)

# Draw edges with specified width and color
nx.draw_networkx_edges(
    G,
    pos,
    width=[G[u][v]['weight'] for u, v in G.edges()],
    edge_color=[edge_color(G[u][v]['weight']) for u, v in G.edges()],
    alpha=0.7,
    connectionstyle="arc3,rad=0.1",
    arrowsize=arrow_size  # Set arrow size
)

plt.title('Association Rules Network Graph')
plt.show()