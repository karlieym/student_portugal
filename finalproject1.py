# -*- coding: utf-8 -*-
"""FinalProject1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfyyfqdJ5tE6i-y6mIheVPRIAoU9cCvK
"""

#upload data and look the data structure
import pandas as pd
Mat_G=pd.read_csv('/content/student-mat.csv')
Por_G=pd.read_csv('/content/student-por.csv')
print(Mat_G.shape)
print(Por_G.shape)

#data cleaning
print(Mat_G.isnull().sum())
print(Por_G.isnull().sum())#it seems there is no need to clean data at this stage

Mat_G.describe()

Por_G.describe()

#visualize distributions of different variables
import seaborn as sns # visualize
import matplotlib.pyplot as plt

# Calculate the number of rows needed to display the subplots
num_cols = len(Por_G.columns)
num_rows = (num_cols - 1) // 4 + 1

# Create a figure and subplots for each column
fig, axes = plt.subplots(nrows=num_rows, ncols=4, figsize=(16, 4*num_rows))

# Plot the distribution graphs for each column
for i, column in enumerate(Por_G.columns):
    row_idx = i // 4
    col_idx = i % 4
    axes[row_idx, col_idx].hist(Por_G[column], bins=20, density=True, alpha=0.6)
    axes[row_idx, col_idx].set_title(f'Distribution of {column}')
    axes[row_idx, col_idx].set_xlabel(column)
    axes[row_idx, col_idx].set_ylabel('Density')

# Adjust the spacing between subplots for better presentation
plt.tight_layout()

# Show the plots
plt.show()

# Calculate the number of rows needed to display the subplots
num_cols = len(Mat_G.columns)
num_rows = (num_cols - 1) // 4 + 1

# Create a figure and subplots for each column
fig, axes = plt.subplots(nrows=num_rows, ncols=4, figsize=(16, 4*num_rows))

# Plot the distribution graphs for each column
for i, column in enumerate(Por_G.columns):
    row_idx = i // 4
    col_idx = i % 4
    axes[row_idx, col_idx].hist(Mat_G[column], bins=20, density=True, alpha=0.6)
    axes[row_idx, col_idx].set_title(f'Distribution of {column}')
    axes[row_idx, col_idx].set_xlabel(column)
    axes[row_idx, col_idx].set_ylabel('Density')

# Adjust the spacing between subplots for better presentation
plt.tight_layout()

# Show the plots
plt.show()

#correlatin between features from math dataset

plt.figure(figsize=(15,15))
sns.heatmap(Mat_G.corr(),annot = True,fmt = ".2f",cbar = True,cmap="YlGnBu")
plt.xticks(rotation=90)
plt.yticks(rotation = 0)

#correlatin between features from Portuguese language dataset
import seaborn as sns # visualize
import matplotlib.pyplot as plt
plt.figure(figsize=(15,15))
sns.heatmap(Por_G.corr(),annot = True,fmt = ".2f",cbar = True,cmap="YlGnBu")
plt.xticks(rotation=90)
plt.yticks(rotation = 0)

"""# Classification"""

#Data processing for maching learning
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'schoolsup', 'famsup',
    'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'Mjob',
    'Fjob', 'reason', 'guardian']
numerical_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',
    'freetime', 'goout', 'Dalc', 'Walc', 'health','absences']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
        ])

X = Por_G[categorical_cols + numerical_cols]
X_transformed = preprocessor.fit_transform(X)

ohe_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)

all_columns = ohe_columns.tolist() + numerical_cols

X_transformed_df = pd.DataFrame(X_transformed, columns=all_columns)

X_transformed_df=X_transformed_df.drop(['school_GP','sex_M','address_U','famsize_GT3','Pstatus_T','schoolsup_no','famsup_no','paid_no','activities_no','nursery_no','higher_no','internet_no','romantic_no','Mjob_other','Fjob_other','reason_other','guardian_other'],axis=1)
X_transformed_df.columns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = X_transformed_df
Y1 = Por_G['G3']
#Covert Y1 to binary
def convert_to_binary(note):
    return 1 if note >= 10 else 0
y1_clf =Y1.apply(convert_to_binary)

X_train_por, X_test_por, y_train_por, y_test_por = train_test_split(X, y1_clf, test_size=0.3, random_state=42)

# Create RandomForestClassifier
rf_por_model = RandomForestClassifier(random_state=42)

# Train the model
rf_por_model.fit(X_train_por, y_train_por)

# Predictions on the test set
test_predictions = rf_por_model.predict(X_test_por)

# Evaluate the model
test_accuracy = accuracy_score(y_test_por, test_predictions)

print("Testing Accuracy:", test_accuracy)

# Additional evaluation metrics
print("\nClassification Report on Test Set:\n", classification_report(y_test_por, test_predictions))
print("\nConfusion Matrix on Test Set:\n", confusion_matrix(y_test_por, test_predictions))

import matplotlib.pyplot as plt

# Get feature importances from the trained model
feature_importances = rf_por_model.feature_importances_

# Get the indices of the top ten features
top_ten_indices = feature_importances.argsort()[-10:][::-1]

# Extract the names and importances of the top ten features
top_ten_features = X_transformed_df.columns[top_ten_indices]
top_ten_importances = feature_importances[top_ten_indices]

plt.figure(figsize=(12, 8))
# Plotting only the top ten features
plt.bar(top_ten_features, top_ten_importances)
plt.title('Top Ten Random Forest Feature Importances')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

"""# Clustering"""

cluster_df = pd.concat([X_transformed_df, Por_G['G3']], axis=1)
cluster_df

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
#choose an optimal k
scaled_df=StandardScaler().fit_transform(cluster_df)

kmeans_kwargs={
"init": "random",
"n_init":10,
"random_state":1,
}

se=[]
for k in range(1,11):
  kmeans=KMeans(n_clusters=k,**kmeans_kwargs)
  kmeans.fit(scaled_df)
  se.append(kmeans.inertia_)

plt.plot(range(1,11),se)
plt.title('Elbow Method for Optimal k')
plt.xticks(range(1,11))
plt.xlabel('Number of Clusters')
plt.ylabel('SE')
plt.show()

"""From the graph, we can see the elbow point can be 2 or 4."""

kmeans=KMeans(init="random",n_clusters=4,n_init=10,random_state=1)
kmeans.fit(scaled_df)
kmeans.labels_

pca=PCA(n_components=25)
pca.fit_transform(scaled_df)
pca.explained_variance_ratio_.sum()

#use PCA
pca_por=pca.fit_transform(scaled_df)
kmeans=KMeans(init="random",n_clusters=4,n_init=10,random_state=1).fit(pca_por)
#add the column 'Class' to the data sets
Class=kmeans.labels_
kcat=pd.DataFrame(pca_por,columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20','PC21','PC22','PC23','PC24','PC25'])
kcat['Class']=Class
kcat

import numpy as np
cluster_counts=np.bincount(Class)
cluster_counts

cluster_colors=['greenyellow','violet','gold','skyblue']
for i in range(kmeans.n_clusters):
  cluster_points=kcat[Class==i]
  plt.scatter(cluster_points.iloc[:,0],cluster_points.iloc[:,1],c=cluster_colors[i],label=f'Cluster{i}')
  plt.legend()

dcstr_df = pd.concat([cluster_df,kcat['Class']], axis=1)
dcstr_df

column_to_normalize = 39

scaler = StandardScaler()

column_data = dcstr_df.iloc[:, column_to_normalize].values.reshape(-1, 1)

scaled_column = scaler.fit_transform(column_data)

dcstr_df.iloc[:, column_to_normalize] = scaled_column

dcstr_df

dcstr_df.columns

#using classification to find features in each cluster
##first,trying KNN methods
from sklearn import datasets
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

x = dcstr_df.drop(columns=['Class'])
y=dcstr_df.Class

dt_por_pca=DecisionTreeClassifier().fit(x,y)
features=dt_por_pca.feature_importances_
fi = pd.DataFrame({'names':x.columns, 'fi': features})
fi = fi.sort_values(by='fi',ascending = False)
fi



Fjob_t_C0 = X_transformed_df.loc[Class==0,'Fjob_teacher'].sum()
Fjob_t_C01 = X_transformed_df.loc[Class==1,'Fjob_teacher'].sum()
Fjob_t_C02 = X_transformed_df.loc[Class==2,'Fjob_teacher'].sum()
Fjob_t_C03 = X_transformed_df.loc[Class==3,'Fjob_teacher'].sum()
categories = ['Cluster 0', 'Cluster 1', 'Cluster 2','Cluster 3']
values = [Fjob_t_C0, Fjob_t_C01, Fjob_t_C02,Fjob_t_C03]

plt.bar(categories, values)

plt.title('number of fathers being teachers')
plt.xlabel('Clusters')
plt.ylabel('number')
plt.show()
print(values)

Fjob_health_C0 = X_transformed_df.loc[Class==0,'Fjob_health'].sum()
Fjob_health_C01 = X_transformed_df.loc[Class==1,'Fjob_health'].sum()
Fjob_health_C02 = X_transformed_df.loc[Class==2,'Fjob_health'].sum()
Fjob_health_C03 = X_transformed_df.loc[Class==3,'Fjob_health'].sum()
categories = ['Cluster 0', 'Cluster 1', 'Cluster 2','Cluster3']
values = [Fjob_health_C0, Fjob_health_C01, Fjob_health_C02,Fjob_health_C03]

plt.bar(categories, values)

plt.title('Number of fathers working in health departments')
plt.xlabel('Clusters')
plt.ylabel('Numbers')
plt.show()
print(values)

school_MS_C0 = X_transformed_df.loc[Class==0,'school_MS'].sum()
school_MS_C01 = X_transformed_df.loc[Class==1,'school_MS'].sum()
school_MS_C02 = X_transformed_df.loc[Class==2,'school_MS'].sum()
school_MS_C03 = X_transformed_df.loc[Class==3,'school_MS'].sum()
categories = ['Cluster 0', 'Cluster 1', 'Cluster 2','Cluster3']
values = [school_MS_C0, school_MS_C01, school_MS_C02,school_MS_C03]

plt.bar(categories, values)

plt.title('Number of students in School Mousinho da Silveira')
plt.xlabel('Clusters')
plt.ylabel('Numbers')
plt.show()
print(values)

guardian_mother_C0 = X_transformed_df.loc[Class==0,'guardian_mother'].sum()
guardian_mother_C01 = X_transformed_df.loc[Class==1,'guardian_mother'].sum()
guardian_mother_C02 = X_transformed_df.loc[Class==2,'guardian_mother'].sum()
guardian_mother_C03 = X_transformed_df.loc[Class==3,'guardian_mother'].sum()
categories = ['Cluster 0', 'Cluster 1', 'Cluster 2','Cluster 3']
values = [guardian_mother_C0, guardian_mother_C01, guardian_mother_C02,guardian_mother_C03]

plt.bar(categories, values)

plt.title('number of students guarded by mom')
plt.xlabel('Clusters')
plt.ylabel('number')
plt.show()
print(values)

G3_C0 = Por_G.loc[Class==0,'G3'].mean()
G3_C01 = Por_G.loc[Class==1,'G3'].mean()
G3_C02 = Por_G.loc[Class==2,'G3'].mean()
G3_C03 = Por_G.loc[Class==3,'G3'].mean()
categories = ['Cluster 0', 'Cluster 1', 'Cluster 2','Cluster 3']
values = [G3_C0, G3_C01, G3_C02,G3_C03]

plt.bar(categories, values)

plt.title('mean final score of each cluster')
plt.xlabel('Clusters')
plt.ylabel('number')
plt.show()
print(values)